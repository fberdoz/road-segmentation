{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPxR5rbti2VV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, re\n",
    "from PIL import Image                                 #create image from array\n",
    "from datetime import datetime                         #date for submission file\n",
    "from  scipy import ndimage                            #for image rotation\n",
    "from sklearn.preprocessing import normalize           #for normailzation\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# CNN libraries (keras) for neural networks\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# additional libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#For Google Colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vU5YEHpi2Vc"
   },
   "outputs": [],
   "source": [
    "SEED = 1642\n",
    "PATCH_SIZE = 16\n",
    "NUMBER_TRAIN_IMG = 1 # max 100\n",
    "ROOT = './'\n",
    "DATA_ROOT_DIR = ROOT+ \"Datasets/\"\n",
    "SUBMISSION_DIR = ROOT + \"Submissions/\"\n",
    "PREDICTION_DIR = ROOT + \"Predictions/\"\n",
    "CHECKPOINT_DIR = ROOT + 'Checkpoints/'\n",
    "CONTEXT = 16\n",
    "NB_CLASSES = 2\n",
    "NB_EPOCHS = 1\n",
    "BATCH_SIZE = 75\n",
    "\n",
    "np.random.seed(SEED)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "in8-_6-jyMj5",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * 255).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "def LoadTrainingData(n_img, rootdir=\"Datasets/training/\", printnames=False):\n",
    "    \"\"\" Load the data from the root directory. (a total of n_img images) \"\"\"\n",
    "\n",
    "    image_dir = rootdir + \"images/\"\n",
    "    files = os.listdir(image_dir)\n",
    "\n",
    "    n = min(n_img, len(files)) # Load maximum 20 images\n",
    "    print(\"Loading \" + str(n) + \" train images...\")\n",
    "    imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "\n",
    "    gt_dir = rootdir + \"groundtruth/\"\n",
    "    print(\"Loading \" + str(n) + \" groundtruth images...\")\n",
    "    gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "\n",
    "    if (printnames):\n",
    "        print(\"The loaded images are: \")\n",
    "        for i in range(n):\n",
    "            print(\"    - \" + files[i])\n",
    "    \n",
    "    return imgs, gt_imgs\n",
    "\n",
    "def DataAugmentation(imgs, gt_imgs, angles, sym=False, printinfo=False):\n",
    "    \"\"\"\n",
    "    Augments the data by rotating the image with the angles given in the list <angles>.\n",
    "    If <sym> is true, it also augments the the data with the images obtained by performing a\n",
    "    y axis symmetry.\n",
    "    \n",
    "    The augmented data will present itself as follows: \n",
    "        \n",
    "        [angles[1] rotations, ..., angles[end] rotations,\n",
    "        Y axis symmetry of the angles[1] rotations, ..., Y axis symmetry of the angles[end] rotations]\n",
    " \n",
    "    \"\"\"\n",
    "    n = len(imgs)\n",
    "    \n",
    "    # Creating the augmented version of the images.\n",
    "    aug_imgs = []\n",
    "    aug_gt_imgs = []\n",
    "    \n",
    "    # Rotating the images and adding them to the augmented data list\n",
    "    for theta in angles:\n",
    "        if (printinfo):\n",
    "            print(\"Augmenting the data with the images rotated by\", theta , \"deg.\")\n",
    "        aug_imgs += [BuildRotatedImage(imgs[i], theta) for i in range(n)]\n",
    "        aug_gt_imgs += [BuildRotatedImage(gt_imgs[i], theta) for i in range(n)]  \n",
    "        \n",
    "    # Y symmetry of the images and adding them to the augmented dat list\n",
    "    if (sym):\n",
    "        if (printinfo):\n",
    "            print(\"Augmenting the data with the symmetries\")\n",
    "        n_tmp = len(aug_imgs)\n",
    "        aug_imgs += [np.flip(aug_imgs[i],1) for i in range(n_tmp)]\n",
    "        aug_gt_imgs += [np.flip(aug_gt_imgs[i],1) for i in range(n_tmp)]\n",
    "        \n",
    "    return aug_imgs, aug_gt_imgs\n",
    "\n",
    "\n",
    "def BuildExtendedImage(img, pad):\n",
    "    \"\"\" Create a 3x3 grid of the imput image by mirroring it at the boundaries\"\"\"\n",
    "    \n",
    "    ext_img = cv2.copyMakeBorder(img, pad, pad, pad, pad, cv2.BORDER_REFLECT) \n",
    "\n",
    "    return ext_img\n",
    "\n",
    "def BuildRotatedImage(img, degree):\n",
    "    \"\"\" Return the same image rataded by <degree> degrees. The corners are filled using mirrored boundaries. \"\"\"\n",
    "    \n",
    "    # Improving performance using existing functions for specific angles\n",
    "    if (degree==0):\n",
    "        return img\n",
    "    elif (degree==90):\n",
    "        return np.rot90(img)\n",
    "    elif (degree==180):\n",
    "        return np.rot90(np.rot90(img))\n",
    "    elif (degree==270):\n",
    "        return np.rot90(np.rot90(np.rot90(img)))\n",
    "    else:\n",
    "        h = img.shape[0]\n",
    "        w = img.shape[1]\n",
    "\n",
    "        padh = math.ceil(h/4)\n",
    "        padw = math.ceil(w/4)\n",
    "        pad = max(padh, padw)\n",
    "\n",
    "\n",
    "        # Extend and rotate the image\n",
    "        ext_img = BuildExtendedImage(img, pad)\n",
    "        rot_img = ndimage.rotate(ext_img, degree, reshape=False)\n",
    "\n",
    "        # Taking care of nummerical accuracies (not sure where they come from)\n",
    "\n",
    "        rot_img[rot_img<0] = 0.0\n",
    "        rot_img[rot_img>1] = 1.0\n",
    "\n",
    "        # Crop the image\n",
    "        if (len(img.shape) > 2):\n",
    "            rot_img = rot_img[pad:pad+h, pad:pad+w, :]\n",
    "        else:\n",
    "            rot_img = rot_img[pad:pad+h, pad:pad+w]\n",
    "        \n",
    "        return rot_img\n",
    "\n",
    "def img_crop(im, w, h, c=0):\n",
    "    \"\"\" Personalized version of the img_crop incorporating the option of getting the context (c) around the pataches\"\"\"\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    \n",
    "    # padding the image to access the context of border patches\n",
    "    if is_2d:\n",
    "        pad_im = np.pad(im,((c,c),(c,c)), 'reflect')\n",
    "    else:\n",
    "        pad_im = np.pad(im,((c,c),(c,c),(0,0)), 'reflect')\n",
    "\n",
    "    # cropping the image\n",
    "    for i in range(c,imgheight+c,h):\n",
    "        for j in range(c,imgwidth+c,w):\n",
    "            if is_2d:\n",
    "                im_patch = pad_im[j-c:(j+w)+c, i-c:(i+h)+c]\n",
    "            else:\n",
    "                im_patch = pad_im[j-c:(j+w)+c, i-c:(i+h)+c, :]\n",
    "\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "def ExtractTrainPatch(imgs, gt_imgs, context=0, balancing=True):\n",
    "    \"\"\" Extract patches of size patch_size from the input images.  \"\"\" \n",
    "    n = len(imgs)\n",
    "    \n",
    "    img_patches = [img_crop(imgs[i], PATCH_SIZE, PATCH_SIZE, context) for i in range(n)]\n",
    "    gt_patches = [img_crop(gt_imgs[i], PATCH_SIZE, PATCH_SIZE) for i in range(n)]\n",
    "\n",
    "    # Linearize list of patches\n",
    "    \n",
    "    X = np.asarray([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "    Y = np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def value_to_class(v, foreground_threshold):\n",
    "    df = np.mean(v)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def PrintFeatureStatistics(X, Y):\n",
    "    print('There are ' + str(X.shape[0]) + ' data points (patches)')\n",
    "    print('The size of the patches is ' + str(X.shape[1]) + 'x' \n",
    "          + str(X.shape[2]) + ' = ' + str(X.shape[1]*X.shape[2]) +\" pixels\")\n",
    "\n",
    "    print('Number of classes = ' + str(len(np.unique(Y))))\n",
    "\n",
    "    Y0 = [i for i, j in enumerate(Y) if j == 0]\n",
    "    Y1 = [i for i, j in enumerate(Y) if j == 1]\n",
    "    print('Class 0 (background): ' + str(len(Y0)) + ' samples')\n",
    "    print('Class 1 (signal): ' + str(len(Y1)) + ' samples')\n",
    "    print('Proportion of road: ', len(Y1)/(len(Y1)+len(Y0)))\n",
    "    print('Proportion of background: ', len(Y0)/(len(Y1)+len(Y0)))\n",
    "\n",
    "def NormalizeFeatures(X):\n",
    "    \"\"\"Normalize X which must have shape (num_data_points,num_features)\"\"\"\n",
    "    m = np.mean(X,axis=0)\n",
    "    s = np.std(X,axis=0)\n",
    "    \n",
    "    return (X-m)/s\n",
    "\n",
    "def Balancing(X_train, Y_train):\n",
    "    c0 = 0  # bgrd\n",
    "    c1 = 0  # road\n",
    "    for i in range(len(Y_train)):\n",
    "        if Y_train[i] == 0:\n",
    "            c0 = c0 + 1\n",
    "        else:\n",
    "            c1 = c1 + 1\n",
    "    print('Number of data points per class (before balancing): background = ' + str(c0) + ' road = ' + str(c1))\n",
    "\n",
    "    print('Balancing training data...')\n",
    "    min_c = min(c0, c1)\n",
    "    idx0 = [i for i in range(len(Y_train)) if Y_train[i] == 0]\n",
    "    idx1 = [i for i in range(len(Y_train)) if Y_train[i] == 1]\n",
    "    indices = idx0[0:min_c] + idx1[0:min_c]\n",
    "    new_indices = np.random.permutation(indices)\n",
    "    \n",
    "    X_balanced = X_train[new_indices]\n",
    "    Y_balanced = Y_train[new_indices]\n",
    "\n",
    "    c0 = 0\n",
    "    c1 = 0\n",
    "    for i in range(len(Y_balanced)):\n",
    "        if Y_balanced[i] == 0:\n",
    "            c0 = c0 + 1\n",
    "        else:\n",
    "            c1 = c1 + 1\n",
    "    print('Number of data points per class (after balancing): background = ' + str(c0) + ' road = ' + str(c1))\n",
    "    \n",
    "    return X_balanced, Y_balanced\n",
    "\n",
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    im = np.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            im[j:j+w, i:i+h] = labels[idx]\n",
    "            idx = idx + 1\n",
    "    return im\n",
    "\n",
    "def make_img_overlay(img, predicted_img):\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    color_mask = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "    color_mask[:,:,0] = predicted_img*255\n",
    "\n",
    "    img8 = img_float_to_uint8(img)\n",
    "    background = Image.fromarray(img8, 'RGB').convert(\"RGBA\")\n",
    "    overlay = Image.fromarray(color_mask, 'RGB').convert(\"RGBA\")\n",
    "    new_img = Image.blend(background, overlay, 0.2)\n",
    "    return new_img\n",
    "\n",
    "def TruePositiveRate(tX, Y, logregModel):\n",
    "    \"\"\"Compute the true positive rate of the lgistic regression model logregModel on \n",
    "       the training augmented data tX.\n",
    "    \"\"\"\n",
    "    # Predict on the training set\n",
    "    Y_pred = logregModel.predict(tX)\n",
    "    \n",
    "    # Get non-zeros in prediction and grountruth arrays\n",
    "    Y_predn = np.nonzero(Y_pred)[0]\n",
    "    Yn = np.nonzero(Y)[0]\n",
    "\n",
    "    TPR = len(list(set(Yn) & set(Y_predn))) / float(len(Yn))\n",
    "    return TPR\n",
    "    \n",
    "def Normalize(X, axis=(0,1,2)):\n",
    "\n",
    "    m = np.mean(X, axis)\n",
    "    s = np.std(X, axis)\n",
    "    print(\"Mean before normalization: \", m)\n",
    "    print(\"Std before normalization: \", s)\n",
    "\n",
    "    X_norm = (X - m)/s\n",
    "    print(\"Mean after normalization: \", np.mean(X_norm, axis))\n",
    "    print(\"Std after normalization: \", np.std(X_norm, axis))\n",
    "    return X_norm\n",
    "\n",
    "\n",
    "def PlotHistory(history):\n",
    "    # Plot training & validation accuracy values\n",
    "\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    plt.plot(history.history['val_categorical_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    score = model.evaluate(X_tr, Y_tr, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "def VisualizeCNNPrediction(CNNModel, img_ind):\n",
    "    \n",
    "    test_rootdir = DATA_ROOT_DIR + \"test_set_images/\"\n",
    "    test_files = os.listdir(test_rootdir)\n",
    "    img_path = test_rootdir + test_files[img_ind] + \"/\" + test_files[img_ind] + \".png\"\n",
    "    \n",
    "    img = load_image(img_path)\n",
    "    Xi = ExtractTestPatch(img, CONTEXT)\n",
    "    Yi_prob = CNNModel.predict(Xi, verbose=0)\n",
    "    Yi_pred = np.argmax(Yi_prob, axis=1)\n",
    "    print(\"Number of predicted road patches: \", Yi_pred.sum())\n",
    "    # Display prediction as an image\n",
    "\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    predicted_im = label_to_img(w, h, PATCH_SIZE, PATCH_SIZE, Yi_pred)\n",
    "    cimg = make_img_overlay(img, predicted_im)\n",
    "    fig1, ax = plt.subplots(figsize=(8, 4)) \n",
    "    ax.set_title(\"CNN prediction\")\n",
    "    ax.imshow(cimg, cmap='Greys_r')\n",
    "\n",
    "def binary_to_uint8(img):\n",
    "    rimg = (img * 255).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "def reconstruct_from_labels(image_id, submissionfile):\n",
    "    im = np.zeros((imgwidth, imgheight), dtype=np.uint8)\n",
    "    f = open(submissionfile)\n",
    "    lines = f.readlines()\n",
    "    image_id_str = \"%.3d_\" % image_id\n",
    "    for i in range(1, len(lines)):\n",
    "        line = lines[i]\n",
    "        if not image_id_str in line:\n",
    "            continue\n",
    "\n",
    "        tokens = line.split(\",\")\n",
    "        id = tokens[0]\n",
    "        prediction = int(tokens[1])\n",
    "        tokens = id.split(\"_\")\n",
    "        i = int(tokens[1])\n",
    "        j = int(tokens[2])\n",
    "\n",
    "        je = min(j + w, imgwidth)\n",
    "        ie = min(i + h, imgheight)\n",
    "        if prediction == 0:\n",
    "            adata = np.zeros((w, h))\n",
    "        else:\n",
    "            adata = np.ones((w, h))\n",
    "\n",
    "        im[j:je, i:ie] = binary_to_uint8(adata)\n",
    "\n",
    "    Image.fromarray(im).save(\"prediction_\" + \"%.3d\" % image_id + \".png\")\n",
    "\n",
    "    return im\n",
    "\n",
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mask_to_submission_strings(image_filename):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    img_number = int(re.search(r\"\\d+\", image_filename).group(0))\n",
    "    im = mpimg.imread(image_filename)\n",
    "    patch_size = 16\n",
    "    for j in range(0, im.shape[1], patch_size):\n",
    "        for i in range(0, im.shape[0], patch_size):\n",
    "            patch = im[i : i + patch_size, j : j + patch_size]\n",
    "            label = patch_to_label(patch)\n",
    "            yield (\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, image_filenames):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, \"w\") as f:\n",
    "        f.write(\"id,prediction\\n\")\n",
    "        for fn in image_filenames:\n",
    "            f.writelines(\"{}\\n\".format(s) for s in mask_to_submission_strings(fn))\n",
    "\n",
    "\n",
    "def ExtractTestPatch(img, context=0):\n",
    "    \"\"\" Extract patches of size patch_size from the input image.\"\"\" \n",
    "\n",
    "\n",
    "    img_patches = img_crop(img, PATCH_SIZE, PATCH_SIZE, context)\n",
    "    \n",
    "    # Linearize list of patches\n",
    "    img_patches = np.asarray([img_patches[i]\n",
    "                              for i in range(len(img_patches))])\n",
    "    return img_patches\n",
    "\n",
    "def CreateSubmission(CNNmodel):\n",
    "    \"\"\" Create a submission file using the trained logregModel.\"\"\"\n",
    "    # paths\n",
    "    test_rootdir = DATA_ROOT_DIR + \"test_set_images/\"\n",
    "    test_files = os.listdir(test_rootdir)\n",
    "    \n",
    "    prediction_filenames = []\n",
    "    \n",
    "    # Prediction of all the test images\n",
    "    for i  in range(len(test_files)):\n",
    "        print(\"Predicting image\" + test_files[i] + \"...\")\n",
    "        # Image path of the i-th image\n",
    "        img_path = test_rootdir + test_files[i] + \"/\" + test_files[i] + \".png\"\n",
    "        \n",
    "        # Extraction of the data feature\n",
    "        img = load_image(img_path)\n",
    "        Xi = ExtractTestPatch(img, CONTEXT)\n",
    "        \n",
    "        # Prediction of the i-th image using the trained model logregModel\n",
    "        Yi_prob = CNNmodel.predict(Xi, verbose=0)\n",
    "        Yi_pred = np.argmax(Yi_prob, axis=1)\n",
    "        \n",
    "        # Construction of the mask\n",
    "        w = img.shape[0]\n",
    "        h = img.shape[1]\n",
    "        predicted_mask = label_to_img(w, h, PATCH_SIZE, PATCH_SIZE, Yi_pred)\n",
    "        \n",
    "        # Creating the name for the predicted mask\n",
    "        img_id = int(re.search(r\"\\d+\", test_files[i]).group(0))\n",
    "        prediction_filenames += [PREDICTION_DIR + \"prediction_\" + \"%.3d\" % img_id + \".png\"]\n",
    "        \n",
    "        # Saving the masks in the preddir folder\n",
    "        Image.fromarray(binary_to_uint8(predicted_mask)).save(prediction_filenames[i])  \n",
    "    \n",
    "    # Create unique filename\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime('%H_%M-%d_%m')\n",
    "\n",
    "    # Create a folder in the submssion directory and save the submission and the model in it\n",
    "    os.mkdir(SUBMISSION_DIR + dt_string)\n",
    "    model.save(SUBMISSION_DIR + dt_string + '/' + 'model.h5')\n",
    "\n",
    "    submission_filename = SUBMISSION_DIR  + dt_string + '/' + 'submission_CNN_' + dt_string + '.csv'\n",
    "    \n",
    "    # Create submission\n",
    "    print(\"Creating submission file...\")\n",
    "    masks_to_submission(submission_filename, prediction_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iLMJedei2Vf"
   },
   "source": [
    "### 1) Loading the set of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iow463wPi2Vj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 train images...\n",
      "Loading 1 groundtruth images...\n"
     ]
    }
   ],
   "source": [
    "traindir = DATA_ROOT_DIR + \"training/\"\n",
    "imagedir = traindir + \"images/\"\n",
    "\n",
    "# list with all the available images name\n",
    "files = os.listdir(imagedir) \n",
    "\n",
    "n = NUMBER_TRAIN_IMG\n",
    "imgs, gt_imgs = LoadTrainingData(n, traindir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxIkMnAFi2Vm"
   },
   "source": [
    "### 2) Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lwXSmDIEi2Vr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting the data with the images rotated by 0 deg.\n",
      "Augmenting the data with the images rotated by 45 deg.\n",
      "(Augmented) number of images (and groundtruth):  2\n"
     ]
    }
   ],
   "source": [
    "# Augmenting the data\n",
    "angles = [0, 45]\n",
    "aug_imgs, aug_gt_imgs = DataAugmentation(imgs, gt_imgs, angles, sym=False, printinfo=True)\n",
    "\n",
    "print(\"(Augmented) number of images (and groundtruth): \", len(aug_imgs))\n",
    "\n",
    "#uncomment to see an image\n",
    "#plt.imshow(aug_imgs[5], cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42iG2utmi2Vt"
   },
   "source": [
    "### 3) Extract patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DV_D7odki2Vw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_patches has size  (1250, 48, 48, 3)\n",
      "gt_patches has size  (1250, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "img_patches, gt_patches = ExtractTrainPatch(aug_imgs, aug_gt_imgs, context=CONTEXT, balancing=True)\n",
    "\n",
    "print(\"img_patches has size \", img_patches.shape)\n",
    "print(\"gt_patches has size \", gt_patches.shape)\n",
    "\n",
    "del aug_imgs, aug_gt_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ht3gqtEdi2Vz"
   },
   "source": [
    "### 4) Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBjGEebmi2V3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points per class (before balancing): background = 970 road = 280\n",
      "Balancing training data...\n",
      "Number of data points per class (after balancing): background = 280 road = 280\n",
      "There are 560 data points (patches)\n",
      "The size of the patches is 48x48 = 2304 pixels\n",
      "Number of classes = 2\n",
      "Class 0 (background): 280 samples\n",
      "Class 1 (signal): 280 samples\n",
      "Proportion of road:  0.5\n",
      "Proportion of background:  0.5\n"
     ]
    }
   ],
   "source": [
    "# Compute features for each image patch\n",
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "#X = np.asarray([extract_features(img_patches[i]) for i in range(len(img_patches))])\n",
    "X_tr_raw = np.copy(img_patches)\n",
    "Y_tr_raw = np.asarray([value_to_class(gt_patches[i], foreground_threshold) for i in range(len(gt_patches))])\n",
    "\n",
    "# Balancing the data\n",
    "X_tr_bal, Y_tr_bal = Balancing(X_tr_raw, Y_tr_raw)\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "Y_tr = np_utils.to_categorical(Y_tr_bal, NB_CLASSES)\n",
    "X_tr = np.copy(X_tr_bal).astype('float32')\n",
    "\n",
    "# Print feature statistics\n",
    "PrintFeatureStatistics(X_tr_bal, Y_tr_bal)\n",
    "\n",
    "#Deleting unused variables\n",
    "del X_tr_bal, Y_tr_bal, X_tr_raw, Y_tr_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r4-fcwcCi2WB"
   },
   "source": [
    "### 5) CNN model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Zu76OBEi2WD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 64)        9472      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,878,786\n",
      "Trainable params: 1,878,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "\n",
    "# input size\n",
    "input_shape = (X_tr.shape[1], X_tr.shape[2], X_tr.shape[3])\n",
    "\n",
    "#Reseting the network\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(64, (7, 7), padding='same', input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64, (5, 5), padding='same', input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64, (3, 3), padding='same', input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Convolution2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Convolution2D(256, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e_s1KI8Gi2WK"
   },
   "source": [
    "### 6) CNN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqAT_GPAi2WF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "Train on 448 samples, validate on 112 samples\n",
      "375/448 [========================>.....] - ETA: 4s - loss: 0.6931 - categorical_accuracy: 0.4880\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.56250, saving model to ./Checkpoints/model.hdf5\n",
      "448/448 [==============================] - 26s 59ms/sample - loss: 0.6932 - categorical_accuracy: 0.4933 - val_loss: 0.6929 - val_categorical_accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(CHECKPOINT_DIR + 'model.hdf5', monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "history = model.fit(X_tr, Y_tr, batch_size=BATCH_SIZE, epochs=NB_EPOCHS, validation_split = 0.2, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2lfmxrQi2WK"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.6930097835404533\n",
      "Test accuracy: 0.5642857\n"
     ]
    }
   ],
   "source": [
    "# Training history\n",
    "PlotHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDt9bqBpi2WN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting imagetest_13...\n",
      "Predicting imagetest_14...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-cd18e1d485d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mCreateSubmission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-12a417e70257>\u001b[0m in \u001b[0;36mCreateSubmission\u001b[0;34m(CNNmodel)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# Prediction of the i-th image using the trained model logregModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mYi_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0mYi_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYi_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     return self._model_iteration(\n\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    445\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create submission\n",
    "CreateSubmission(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WFGh3wimi2WP"
   },
   "outputs": [],
   "source": [
    "# Visualize a prediction\n",
    "VisualizeCNNPrediction(model, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kV-KonUVi2WQ"
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CNN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
